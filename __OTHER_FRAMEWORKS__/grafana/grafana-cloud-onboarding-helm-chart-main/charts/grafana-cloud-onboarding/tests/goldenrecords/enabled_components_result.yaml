---
# Source: grafana-cloud-onboarding/charts/alloy-operator/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: test-alloy-operator
  namespace: test-namespace
  labels:
    helm.sh/chart: alloy-operator-0.3.13
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "1.4.0"
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: grafana-cloud-onboarding/charts/beyla/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: test-beyla
  namespace: test-namespace
  labels:
    helm.sh/chart: beyla-1.10.0
    app.kubernetes.io/name: beyla
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "2.7.5"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: beyla
    app.kubernetes.io/component: rbac
automountServiceAccountToken: true
---
# Source: grafana-cloud-onboarding/charts/kepler/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: test-kepler
  namespace: test-namespace
  labels:
    helm.sh/chart: kepler-0.6.1
    app.kubernetes.io/name: kepler
    app.kubernetes.io/component: exporter
    app.kubernetes.io/version: "release-0.8.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: grafana-cloud-onboarding/charts/kube-state-metrics/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-6.4.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "2.17.0"
  name: test-kube-state-metrics
  namespace: test-namespace
---
# Source: grafana-cloud-onboarding/charts/prometheus-node-exporter/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: test-prometheus-node-exporter
  namespace: test-namespace
  labels:
    helm.sh/chart: prometheus-node-exporter-4.49.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "1.10.2"
automountServiceAccountToken: false
---
# Source: grafana-cloud-onboarding/charts/prometheus-windows-exporter/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: test-prometheus-windows-exporter
  namespace: test-namespace
  labels:
    helm.sh/chart: prometheus-windows-exporter-0.12.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-windows-exporter
    app.kubernetes.io/name: prometheus-windows-exporter
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "0.31.3"
---
# Source: grafana-cloud-onboarding/templates/fleet_management_secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: "grafana-cloud-fleet-management-test-grafana-cloud-onboarding"
  namespace: "test-namespace"
type: Opaque
data:
  username: "MTIzNDU2"
  password: "Z2xjX3RoaXNpc215cGFzc3dvcmQ="
---
# Source: grafana-cloud-onboarding/charts/beyla/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: test-beyla
  namespace: test-namespace
  labels:
    helm.sh/chart: beyla-1.10.0
    app.kubernetes.io/name: beyla
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "2.7.5"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: beyla
    app.kubernetes.io/component: config
data:
  beyla-config.yml: |
    discovery:
      instrument:
        - k8s_namespace: "*"
    attributes:
      kubernetes:
        enable: true
    filter:
      network:
        k8s_dst_owner_name:
          not_match: '{kube*,*jaeger-agent*,*prometheus*,*promtail*,*grafana-agent*}'
        k8s_src_owner_name:
          not_match: '{kube*,*jaeger-agent*,*prometheus*,*promtail*,*grafana-agent*}'
    prometheus_export:
      path: /metrics
      port: 9090
---
# Source: grafana-cloud-onboarding/charts/prometheus-windows-exporter/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: test-prometheus-windows-exporter
  namespace: test-namespace
  labels:
    helm.sh/chart: prometheus-windows-exporter-0.12.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-windows-exporter
    app.kubernetes.io/name: prometheus-windows-exporter
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "0.31.3"
data:
  config.yml: |
    collectors:
      enabled: '[defaults],memory,container'
---
# Source: grafana-cloud-onboarding/charts/alloy-operator/templates/rbac/alloy-manager.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: test-alloy-operator-alloy-manager
  labels:
    helm.sh/chart: alloy-operator-0.3.13
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "1.4.0"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - collectors.grafana.com
    resources:
      - alloys
      - alloys/status
      - alloys/finalizers
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
---
# Source: grafana-cloud-onboarding/charts/alloy-operator/templates/rbac/alloy-objects.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: test-alloy-operator
rules:
  # Rules which allow the management of ConfigMaps, ServiceAccounts, and Services.
  - apiGroups: [""]
    resources: ["configmaps", "secrets", "serviceaccounts", "services"]
    verbs: ["*"]
  # Rules which allow the management of DaemonSets, Deployments, and StatefulSets.
  - apiGroups: ["apps"]
    resources: ["daemonsets", "deployments", "statefulsets"]
    verbs: ["*"]
  # Rules which allow the management of Horizontal Pod Autoscalers.
  - apiGroups: ["autoscaling"]
    resources: ["horizontalpodautoscalers"]
    verbs: ["*"]
  # Rules which allow the management of Ingresses and NetworkPolicies.
  - apiGroups: ["networking.k8s.io"]
    resources: ["ingresses", "networkpolicies"]
    verbs: ["*"]
  # Rules which allow the management of PodDisruptionBudgets.
  - apiGroups: ["policy"]
    resources: ["poddisruptionbudgets"]
    verbs: ["*"]
  # Rules which allow the management of ClusterRoles, ClusterRoleBindings, Roles, and RoleBindings.
  - apiGroups: ["rbac.authorization.k8s.io"]
    resources: ["clusterroles", "clusterrolebindings", "roles", "rolebindings"]
    verbs: ["*"]
---
# Source: grafana-cloud-onboarding/charts/beyla/templates/cluster-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: test-beyla
  labels:
    helm.sh/chart: beyla-1.10.0
    app.kubernetes.io/name: beyla
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "2.7.5"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: beyla
    app.kubernetes.io/component: rbac
rules:
  - apiGroups: [ "apps" ]
    resources: [ "replicasets" ]
    verbs: [ "list", "watch" ]
  - apiGroups: [ "" ]
    resources: [ "pods", "services", "nodes" ]
    verbs: [ "list", "watch", "get" ]
---
# Source: grafana-cloud-onboarding/charts/kepler/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: test-kepler-clusterrole
rules:
  - apiGroups: [""]
    resources:
      - nodes/metrics # access /metrics/resource
      - nodes/proxy
      - nodes/stats
      - pods
    verbs:
      - get
      - watch
      - list
---
# Source: grafana-cloud-onboarding/charts/kube-state-metrics/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-6.4.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "2.17.0"
  name: test-kube-state-metrics
rules:

- apiGroups: ["certificates.k8s.io"]
  resources:
  - certificatesigningrequests
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - cronjobs
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - daemonsets
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - deployments
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - endpoints
  verbs: ["list", "watch"]

- apiGroups: ["autoscaling"]
  resources:
  - horizontalpodautoscalers
  verbs: ["list", "watch"]

- apiGroups: ["networking.k8s.io"]
  resources:
  - ingresses
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - jobs
  verbs: ["list", "watch"]

- apiGroups: ["coordination.k8s.io"]
  resources:
  - leases
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - limitranges
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - mutatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - namespaces
  verbs: ["list", "watch"]

- apiGroups: ["networking.k8s.io"]
  resources:
  - networkpolicies
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - nodes
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumeclaims
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumes
  verbs: ["list", "watch"]

- apiGroups: ["policy"]
  resources:
    - poddisruptionbudgets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - pods
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - replicasets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - replicationcontrollers
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - resourcequotas
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - secrets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - services
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - statefulsets
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - storageclasses
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - validatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - volumeattachments
  verbs: ["list", "watch"]
---
# Source: grafana-cloud-onboarding/charts/alloy-operator/templates/rbac/alloy-manager.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: test-alloy-operator-alloy-manager
  labels:
    helm.sh/chart: alloy-operator-0.3.13
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "1.4.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: test-alloy-operator-alloy-manager
subjects:
  - kind: ServiceAccount
    name: test-alloy-operator
    namespace: test-namespace
---
# Source: grafana-cloud-onboarding/charts/alloy-operator/templates/rbac/alloy-objects.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: test-alloy-operator
  labels:
    helm.sh/chart: alloy-operator-0.3.13
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "1.4.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: test-alloy-operator
subjects:
  - kind: ServiceAccount
    name: test-alloy-operator
    namespace: test-namespace
---
# Source: grafana-cloud-onboarding/charts/beyla/templates/cluster-role-binding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: test-beyla
  labels:
    helm.sh/chart: beyla-1.10.0
    app.kubernetes.io/name: beyla
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "2.7.5"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: beyla
    app.kubernetes.io/component: rbac
subjects:
  - kind: ServiceAccount
    name: test-beyla
    namespace: test-namespace
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: test-beyla
---
# Source: grafana-cloud-onboarding/charts/kepler/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: test-kepler-clusterrole-binding
roleRef:
  kind: ClusterRole
  name: test-kepler-clusterrole
  apiGroup: "rbac.authorization.k8s.io"
subjects:
  - kind: ServiceAccount
    name: test-kepler
    namespace: test-namespace
---
# Source: grafana-cloud-onboarding/charts/kube-state-metrics/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-6.4.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "2.17.0"
  name: test-kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: test-kube-state-metrics
subjects:
- kind: ServiceAccount
  name: test-kube-state-metrics
  namespace: test-namespace
---
# Source: grafana-cloud-onboarding/charts/alloy-operator/templates/rbac/leader-election.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: test-alloy-operator-leader-election-role
  namespace: test-namespace
  labels:
    helm.sh/chart: alloy-operator-0.3.13
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "1.4.0"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
# Source: grafana-cloud-onboarding/charts/alloy-operator/templates/rbac/leader-election.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: test-alloy-operator-leader-election-rolebinding
  namespace: test-namespace
  labels:
    helm.sh/chart: alloy-operator-0.3.13
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "1.4.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: test-alloy-operator-leader-election-role
subjects:
  - kind: ServiceAccount
    name: test-alloy-operator
    namespace: test-namespace
---
# Source: grafana-cloud-onboarding/charts/alloy-operator/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: test-alloy-operator
  namespace: test-namespace
  labels:
    helm.sh/chart: alloy-operator-0.3.13
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "1.4.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8081
      targetPort: http
      protocol: TCP
    - name: metrics
      port: 8082
      targetPort: metrics
      protocol: TCP
  selector:
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: test
---
# Source: grafana-cloud-onboarding/charts/beyla/templates/cache-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: beyla-k8s-cache
  namespace: test-namespace
  labels:
    helm.sh/chart: beyla-1.10.0
    app.kubernetes.io/name: beyla-k8s-cache
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "2.7.5"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: beyla
    app.kubernetes.io/component: networking
spec:
  ports:
    - port: 50055
      protocol: TCP
      targetPort: grpc
      name: grpc
  selector:
    app.kubernetes.io/name: beyla-k8s-cache
---
# Source: grafana-cloud-onboarding/charts/kepler/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: test-kepler
  namespace: test-namespace
  labels:
    helm.sh/chart: kepler-0.6.1
    app.kubernetes.io/name: kepler
    app.kubernetes.io/component: exporter
    app.kubernetes.io/version: "release-0.8.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 9102
      targetPort: http
      protocol: TCP
  selector:
    app.kubernetes.io/name: kepler
    app.kubernetes.io/component: exporter
---
# Source: grafana-cloud-onboarding/charts/kube-state-metrics/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: test-kube-state-metrics
  namespace: test-namespace
  labels:    
    helm.sh/chart: kube-state-metrics-6.4.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "2.17.0"
  annotations:
    prometheus.io/scrape: 'true'
spec:
  type: "ClusterIP"
  ports:
  - name: http
    protocol: TCP
    port: 8080
    targetPort: http
  
  selector:    
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: test
---
# Source: grafana-cloud-onboarding/charts/prometheus-node-exporter/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: test-prometheus-node-exporter
  namespace: test-namespace
  labels:
    helm.sh/chart: prometheus-node-exporter-4.49.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "1.10.2"
  annotations:
    prometheus.io/scrape: "true"
spec:
  type: ClusterIP
  ports:
    - port: 9100
      targetPort: 9100
      protocol: TCP
      name: metrics
  selector:
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: test
---
# Source: grafana-cloud-onboarding/charts/prometheus-windows-exporter/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: test-prometheus-windows-exporter
  namespace: test-namespace
  labels:
    helm.sh/chart: prometheus-windows-exporter-0.12.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-windows-exporter
    app.kubernetes.io/name: prometheus-windows-exporter
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "0.31.3"
spec:
  type: ClusterIP
  ports:
    - port: 9182
      targetPort: metrics
      protocol: TCP
      appProtocol: http
      name: metrics
  selector:
    app.kubernetes.io/name: prometheus-windows-exporter
    app.kubernetes.io/instance: test
---
# Source: grafana-cloud-onboarding/charts/beyla/templates/daemon-set.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: test-beyla
  namespace: test-namespace
  labels:
    helm.sh/chart: beyla-1.10.0
    app.kubernetes.io/name: beyla
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "2.7.5"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: beyla
    app.kubernetes.io/component: workload
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: beyla
      app.kubernetes.io/instance: test
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: cc8d2fae94d56f52e93e2df244f74145c92867cc18ba0919f560c4a0090dca99
      labels:
        helm.sh/chart: beyla-1.10.0
        app.kubernetes.io/name: beyla
        app.kubernetes.io/instance: test
        app.kubernetes.io/version: "2.7.5"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/part-of: beyla
        app.kubernetes.io/component: workload
    spec:
      serviceAccountName: test-beyla
      hostPID: true
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      containers:
        - name: beyla
          image: docker.io/grafana/beyla:2.7.5
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: true
          ports:
          - name: metrics
            containerPort: 9090
            protocol: TCP
          env:
            - name: BEYLA_CONFIG_PATH
              value: "/etc/beyla/config/beyla-config.yml"
            - name: BEYLA_KUBE_META_CACHE_ADDRESS
              value: beyla-k8s-cache:50055
          volumeMounts:
            - mountPath: /etc/beyla/config
              name: beyla-config
            - mountPath: /sys/fs/cgroup
              name: cgroup
      volumes:
        - name: beyla-config
          configMap:
            name: test-beyla
        - name: cgroup
          hostPath:
            path: /sys/fs/cgroup
---
# Source: grafana-cloud-onboarding/charts/kepler/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: test-kepler
  namespace: test-namespace
  labels:
    helm.sh/chart: kepler-0.6.1
    app.kubernetes.io/name: kepler
    app.kubernetes.io/component: exporter
    app.kubernetes.io/version: "release-0.8.0"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: kepler
      app.kubernetes.io/component: exporter
  template:
    metadata:
      labels:
        app.kubernetes.io/name: kepler
        app.kubernetes.io/component: exporter
    spec:
      hostNetwork: true
      serviceAccountName: test-kepler
      containers:
      - name: kepler-exporter
        image: "quay.io/sustainable_computing_io/kepler:release-0.8.0"
        imagePullPolicy: Always
        securityContext:
            privileged: true
        args:
          - -v=$(KEPLER_LOG_LEVEL)
        env:
          - name: NODE_IP
            valueFrom:
              fieldRef:
                fieldPath: status.hostIP
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: METRIC_PATH
            value: "/metrics"
          - name: BIND_ADDRESS
            value: "0.0.0.0:9102"
          - name: "CGROUP_METRICS"
            value: "*"
          - name: "CPU_ARCH_OVERRIDE"
            value: ""
          - name: "ENABLE_EBPF_CGROUPID"
            value: "true"
          - name: "ENABLE_GPU"
            value: "true"
          - name: "ENABLE_PROCESS_METRICS"
            value: "false"
          - name: "ENABLE_QAT"
            value: "false"
          - name: "EXPOSE_CGROUP_METRICS"
            value: "false"
          - name: "EXPOSE_HW_COUNTER_METRICS"
            value: "true"
          - name: "EXPOSE_IRQ_COUNTER_METRICS"
            value: "true"
          - name: "KEPLER_LOG_LEVEL"
            value: "1"
        ports:
        - containerPort: 9102
          hostPort: 9102
          name: http
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /healthz
            port: http
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 60
          successThreshold: 1
          timeoutSeconds: 10
        readinessProbe:
          httpGet:
            path: /healthz
            port: http
            scheme: HTTP
          initialDelaySeconds: 10
        startupProbe:
          httpGet:
            path: /healthz
            port: http
            scheme: HTTP
          initialDelaySeconds: 1
        volumeMounts:
          - name: lib-modules
            mountPath: /lib/modules
          - name: tracing
            mountPath: /sys
          - name: proc
            mountPath: /proc
          - name: config-dir
            mountPath: /etc/kepler
          - name: usr-src
            mountPath: /usr/src
      volumes:
        - name: lib-modules
          hostPath:
            path: /lib/modules
            type: DirectoryOrCreate
        - name: tracing
          hostPath:
            path: /sys
            type: Directory
        - name: proc
          hostPath:
            path: /proc
            type: Directory
        - name: config-dir
          emptyDir:
            sizeLimit: 100Ki
        - name: usr-src
          hostPath:
            path: /usr/src
            type: Directory
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
---
# Source: grafana-cloud-onboarding/charts/prometheus-node-exporter/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: test-prometheus-node-exporter
  namespace: test-namespace
  labels:
    helm.sh/chart: prometheus-node-exporter-4.49.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "1.10.2"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: prometheus-node-exporter
      app.kubernetes.io/instance: test
  revisionHistoryLimit: 10
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        helm.sh/chart: prometheus-node-exporter-4.49.1
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: prometheus-node-exporter
        app.kubernetes.io/name: prometheus-node-exporter
        app.kubernetes.io/instance: test
        app.kubernetes.io/version: "1.10.2"
    spec:
      automountServiceAccountToken: false
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      serviceAccountName: test-prometheus-node-exporter
      containers:
        - name: node-exporter
          image: quay.io/prometheus/node-exporter:v1.10.2
          imagePullPolicy: IfNotPresent
          args:
            - --path.procfs=/host/proc
            - --path.sysfs=/host/sys
            - --path.rootfs=/host/root
            - --path.udev.data=/host/root/run/udev/data
            - --web.listen-address=[$(HOST_IP)]:9100
          securityContext:
            readOnlyRootFilesystem: true
          env:
            - name: HOST_IP
              value: 0.0.0.0
          ports:
            - name: metrics
              containerPort: 9100
              protocol: TCP
          livenessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              path: /
              port: metrics
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              path: /
              port: metrics
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          volumeMounts:
            - name: proc
              mountPath: /host/proc
              readOnly:  true
            - name: sys
              mountPath: /host/sys
              readOnly: true
            - name: root
              mountPath: /host/root
              mountPropagation: HostToContainer
              readOnly: true
      hostNetwork: true
      hostPID: true
      hostIPC: false
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: eks.amazonaws.com/compute-type
                operator: NotIn
                values:
                - fargate
              - key: type
                operator: NotIn
                values:
                - virtual-kubelet
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
        - effect: NoSchedule
          operator: Exists
      volumes:
        - name: proc
          hostPath:
            path: /proc
        - name: sys
          hostPath:
            path: /sys
        - name: root
          hostPath:
            path: /
---
# Source: grafana-cloud-onboarding/charts/prometheus-windows-exporter/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: test-prometheus-windows-exporter
  namespace: test-namespace
  labels:
    helm.sh/chart: prometheus-windows-exporter-0.12.2
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-windows-exporter
    app.kubernetes.io/name: prometheus-windows-exporter
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "0.31.3"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: prometheus-windows-exporter
      app.kubernetes.io/instance: test
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        helm.sh/chart: prometheus-windows-exporter-0.12.2
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: prometheus-windows-exporter
        app.kubernetes.io/name: prometheus-windows-exporter
        app.kubernetes.io/instance: test
        app.kubernetes.io/version: "0.31.3"
    spec:
      automountServiceAccountToken: false
      securityContext:
        windowsOptions:
          hostProcess: true
          runAsUserName: NT AUTHORITY\system
      initContainers:
        - name: configure-firewall
          image: ghcr.io/prometheus-community/windows-exporter:0.31.3
          command: [ "powershell" ]
          args: [ "New-NetFirewallRule", "-DisplayName", "'windows-exporter'", "-Direction", "inbound", "-Profile", "Any", "-Action", "Allow", "-LocalPort", "9182", "-Protocol", "TCP" ]
      serviceAccountName: test-prometheus-windows-exporter
      containers:
        - name: windows-exporter
          image: ghcr.io/prometheus-community/windows-exporter:0.31.3
          imagePullPolicy: IfNotPresent
          args:
            - --config.file=%CONTAINER_SANDBOX_MOUNT_POINT%/config.yml
            - --collector.textfile.directories=%CONTAINER_SANDBOX_MOUNT_POINT%
            - --web.listen-address=:9182
          env:
          ports:
            - name: metrics
              containerPort: 9182
              protocol: TCP
          livenessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              path: /health
              port: 9182
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe:
            failureThreshold: 3
            httpGet:
              httpHeaders:
              path: /health
              port: 9182
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          volumeMounts:
            - name: config
              mountPath: /config.yml
              subPath: config.yml
      hostNetwork: true
      hostPID: true
      nodeSelector:
        kubernetes.io/os: windows
      tolerations:
        - effect: NoSchedule
          operator: Exists
      volumes:
        - name: config
          configMap:
            name: test-prometheus-windows-exporter
---
# Source: grafana-cloud-onboarding/charts/alloy-operator/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-alloy-operator
  namespace: test-namespace
  labels:
    helm.sh/chart: alloy-operator-0.3.13
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "1.4.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: alloy-operator
      app.kubernetes.io/instance: test
  template:
    metadata:
      labels:
        helm.sh/chart: alloy-operator-0.3.13
        app.kubernetes.io/name: alloy-operator
        app.kubernetes.io/instance: test
        app.kubernetes.io/version: "1.4.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: test-alloy-operator
      securityContext:
        fsGroup: 1000
        runAsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
        seccompProfile:
          type: RuntimeDefault
      containers:
        - name: alloy-operator
          image: "ghcr.io/grafana/alloy-operator:1.4.0"
          imagePullPolicy: IfNotPresent
          args:
            - --health-probe-bind-address=:8081
            - --metrics-bind-address=:8082
            - --leader-elect
            - --leader-election-id=test-alloy-operator

          ports:
            - name: http
              containerPort: 8081
              protocol: TCP
            - name: metrics
              containerPort: 8082
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8081
            initialDelaySeconds: 15
            periodSeconds: 20
          readinessProbe:
            httpGet:
              path: /readyz
              port: 8081
            initialDelaySeconds: 5
            periodSeconds: 10
          resources:
            limits: {}
            requests: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
      nodeSelector:
        kubernetes.io/os: linux
---
# Source: grafana-cloud-onboarding/charts/beyla/templates/cache-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: beyla-k8s-cache
  namespace: test-namespace
  labels:
    helm.sh/chart: beyla-1.10.0
    app.kubernetes.io/name: beyla-k8s-cache
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "2.7.5"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: beyla
    app.kubernetes.io/component: workload
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: beyla-k8s-cache
  template:
    metadata:
      labels:
        helm.sh/chart: beyla-1.10.0
        app.kubernetes.io/name: beyla-k8s-cache
        app.kubernetes.io/instance: test
        app.kubernetes.io/version: "2.7.5"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/part-of: beyla
    spec:
      serviceAccountName: test-beyla
      containers:
        - name: beyla-cache
          image: docker.io/grafana/beyla-k8s-cache:2
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 50055
              protocol: TCP
              name: grpc
          resources:
            limits:
              cpu: "3.0"
              memory: 2Gi
            requests:
              cpu: "0.1"
              memory: 256Mi
          env:
            - name: BEYLA_K8S_CACHE_PORT
              value: "50055"
---
# Source: grafana-cloud-onboarding/charts/kube-state-metrics/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-kube-state-metrics
  namespace: test-namespace
  labels:    
    helm.sh/chart: kube-state-metrics-6.4.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: test
    app.kubernetes.io/version: "2.17.0"
spec:
  selector:
    matchLabels:      
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/instance: test
  replicas: 1
  strategy:
    type: RollingUpdate
  revisionHistoryLimit: 10
  template:
    metadata:
      labels:        
        helm.sh/chart: kube-state-metrics-6.4.1
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: kube-state-metrics
        app.kubernetes.io/name: kube-state-metrics
        app.kubernetes.io/instance: test
        app.kubernetes.io/version: "2.17.0"
    spec:
      automountServiceAccountToken: true
      hostNetwork: false
      serviceAccountName: test-kube-state-metrics
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
        seccompProfile:
          type: RuntimeDefault
      dnsPolicy: ClusterFirst
      containers:
      - name: kube-state-metrics
        args:
        - --port=8080
        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
        imagePullPolicy: IfNotPresent
        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0
        ports:
        - containerPort: 8080
          name: http
        livenessProbe:
          failureThreshold: 3
          httpGet:
            httpHeaders:
            path: /livez
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        readinessProbe:
          failureThreshold: 3
          httpGet:
            httpHeaders:
            path: /readyz
            port: 8081
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        resources:
          {}
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
---
# Source: grafana-cloud-onboarding/templates/alloy.yaml
apiVersion: collectors.grafana.com/v1alpha1
kind: Alloy
metadata:
  name: test-alloy-daemon
  namespace: test-namespace
spec: 
  alloy:
    clustering:
      enabled: false
      name: ""
      portName: http
    configMap:
      content: |-
        remote.kubernetes.secret "grafana_cloud_fleet_management" {
          name      = "grafana-cloud-fleet-management-test-grafana-cloud-onboarding"
          namespace = "test-namespace"
        }
        remotecfg {
          id = sys.env("GCLOUD_FM_COLLECTOR_ID")
          url = "https://fleet.grafana.com"
          basic_auth {
            username = convert.nonsensitive(remote.kubernetes.secret.grafana_cloud_fleet_management.data["username"])
            password = remote.kubernetes.secret.grafana_cloud_fleet_management.data["password"]
          }
          poll_frequency = "30s"
          attributes = {
            "cluster" = "alloy-example",
            "namespace" = "test-namespace",
            "platform" = "kubernetes",
            "release" = "test",
            "source" = "grafana-cloud-onboarding",
            "sourceVersion" = "0.4.0",
            "workloadName" = "alloy-daemon",
            "workloadType" = "daemonset",
          }
        }
        logging {
          level = "info"
        }
        livedebugging {
          enabled = true
        }
      create: true
      key: null
      name: null
    enableReporting: true
    envFrom: []
    extraArgs: []
    extraEnv:
    - name: CLUSTER_NAME
      value: alloy-example
    - name: GCLOUD_RW_API_KEY
      valueFrom:
        secretKeyRef:
          key: password
          name: grafana-cloud-fleet-management-test-grafana-cloud-onboarding
          namespace: test-namespace
    - name: NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
    - name: POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    - name: GCLOUD_FM_COLLECTOR_ID
      value: alloy-$(CLUSTER_NAME)-$(NAMESPACE)-$(POD_NAME)
    extraPorts: []
    hostAliases: []
    lifecycle: {}
    listenAddr: 0.0.0.0
    listenPort: 12345
    listenScheme: HTTP
    livenessProbe: {}
    mounts:
      dockercontainers: false
      extra:
      - mountPath: /var/lib/alloy
        name: alloy-storage
        readOnly: false
      varlog: true
    resources: {}
    securityContext:
      privileged: true
    stabilityLevel: public-preview
    storagePath: /var/lib/alloy
    uiPathPrefix: /
  configReloader:
    customArgs: []
    enabled: true
    image:
      digest: ""
      registry: quay.io
      repository: prometheus-operator/prometheus-config-reloader
      tag: v0.81.0
    resources:
      requests:
        cpu: 10m
        memory: 50Mi
    securityContext: {}
  controller:
    affinity: {}
    autoscaling:
      enabled: false
      horizontal:
        enabled: false
        maxReplicas: 5
        minReplicas: 1
        scaleDown:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 300
        scaleUp:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 0
        targetCPUUtilizationPercentage: 0
        targetMemoryUtilizationPercentage: 80
      maxReplicas: 5
      minReplicas: 1
      scaleDown:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 300
      scaleUp:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 0
      targetCPUUtilizationPercentage: 0
      targetMemoryUtilizationPercentage: 80
      vertical:
        enabled: false
        recommenders: []
        resourcePolicy:
          containerPolicies:
          - containerName: alloy
            controlledResources:
            - cpu
            - memory
            controlledValues: RequestsAndLimits
            maxAllowed: {}
            minAllowed: {}
        updatePolicy: null
    dnsPolicy: ClusterFirstWithHostNet
    enableStatefulSetAutoDeletePVC: false
    extraAnnotations: {}
    extraContainers: []
    hostNetwork: true
    hostPID: true
    initContainers: []
    nodeSelector: {}
    parallelRollout: true
    podAnnotations: {}
    podDisruptionBudget:
      enabled: false
      maxUnavailable: null
      minAvailable: null
    podLabels: {}
    priorityClassName: ""
    replicas: 1
    terminationGracePeriodSeconds: null
    tolerations: []
    topologySpreadConstraints: []
    type: daemonset
    updateStrategy: {}
    volumeClaimTemplates: []
    volumes:
      extra:
      - hostPath:
          path: /var/lib/alloy
          type: DirectoryOrCreate
        name: alloy-storage
  crds:
    create: false
  extraObjects: []
  global:
    image:
      pullSecrets: []
      registry: ""
    podSecurityContext: {}
  image:
    digest: null
    pullPolicy: IfNotPresent
    pullSecrets: []
    registry: docker.io
    repository: grafana/alloy
    tag: null
  ingress:
    annotations: {}
    enabled: false
    extraPaths: []
    faroPort: 12347
    hosts:
    - chart-example.local
    labels: {}
    path: /
    pathType: Prefix
    tls: []
  nameOverride: alloy-daemon
  rbac:
    create: true
  service:
    annotations: {}
    clusterIP: ""
    enabled: true
    internalTrafficPolicy: Cluster
    nodePort: 31128
    type: ClusterIP
  serviceAccount:
    additionalLabels: {}
    annotations: {}
    automountServiceAccountToken: true
    create: true
    name: null
  serviceMonitor:
    additionalLabels: {}
    enabled: false
    interval: ""
    metricRelabelings: []
    relabelings: []
    tlsConfig: {}
---
# Source: grafana-cloud-onboarding/templates/hooks/post-install_add-finalizer.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: test-grafana-cloud-onboarding-add-finalizer
  namespace: test-namespace
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "5"
---
# Source: grafana-cloud-onboarding/templates/hooks/pre-delete_remove-alloy-and-finalizer.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: test-grafana-cloud-onboarding-remove-alloy-and-finalizer
  namespace: test-namespace
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
---
# Source: grafana-cloud-onboarding/templates/hooks/post-install_add-finalizer.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: test-grafana-cloud-onboarding-add-finalizer
  namespace: test-namespace
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "5"
rules:
  - apiGroups: ["apps"]
    resources: ["deployments"]
    verbs: ["get", "list", "watch", "patch"]
---
# Source: grafana-cloud-onboarding/templates/hooks/pre-delete_remove-alloy-and-finalizer.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: test-grafana-cloud-onboarding-remove-alloy-and-finalizer
  namespace: test-namespace
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
rules:
  - apiGroups: ["apps"]
    resources: ["deployments"]
    verbs: ["get", "list", "watch", "patch"]
  - apiGroups: ["collectors.grafana.com"]
    resources: ["alloys"]
    verbs: ["get", "list", "watch", "delete"]
---
# Source: grafana-cloud-onboarding/templates/hooks/post-install_add-finalizer.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: test-grafana-cloud-onboarding-add-finalizer
  namespace: test-namespace
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "5"
subjects:
  - kind: ServiceAccount
    name: test-grafana-cloud-onboarding-add-finalizer
    namespace: test-namespace
roleRef:
  kind: Role
  name: test-grafana-cloud-onboarding-add-finalizer
  apiGroup: rbac.authorization.k8s.io
---
# Source: grafana-cloud-onboarding/templates/hooks/pre-delete_remove-alloy-and-finalizer.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: test-grafana-cloud-onboarding-remove-alloy-and-finalizer
  namespace: test-namespace
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
subjects:
  - kind: ServiceAccount
    name: test-grafana-cloud-onboarding-remove-alloy-and-finalizer
    namespace: test-namespace
roleRef:
  kind: Role
  name: test-grafana-cloud-onboarding-remove-alloy-and-finalizer
  apiGroup: rbac.authorization.k8s.io
---
# Source: grafana-cloud-onboarding/templates/hooks/post-install_add-finalizer.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: test-grafana-cloud-onboarding-add-finalizer
  namespace: test-namespace
  labels:
    app.kubernetes.io/name: test-grafana-cloud-onboarding-add-finalizer
    app.kubernetes.io/instance: test
    helm.sh/chart: grafana-cloud-onboarding-0.4.0
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-weight: "15"
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
spec:
  ttlSecondsAfterFinished: 300
  backoffLimit: 3
  template:
    metadata:
      name: test-grafana-cloud-onboarding-add-finalizer
      labels:
        app.kubernetes.io/name: test-grafana-cloud-onboarding
        app.kubernetes.io/instance: test
        linkerd.io/inject: disabled
        sidecar.istio.io/inject: "false"
    spec:
      restartPolicy: Never
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: test-grafana-cloud-onboarding-add-finalizer
      containers:
        - name: add-finalizers
          image: "ghcr.io/grafana/helm-chart-toolbox-kubectl:0.1.1"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - -ce
            - |
              kubectl patch \
                --namespace=test-namespace \
                --patch='{"metadata":{"finalizers":["k8s.grafana.com/finalizer"]}}' \
                deployment/test-alloy-operator
          securityContext:
            runAsNonRoot: true
            runAsUser: 4242
            allowPrivilegeEscalation: false
            capabilities:
              drop: ["ALL"]
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
---
# Source: grafana-cloud-onboarding/templates/hooks/pre-delete_remove-alloy-and-finalizer.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: test-grafana-cloud-onboarding-remove-alloy-and-finalizer
  namespace: test-namespace
  labels:
    app.kubernetes.io/name: test-grafana-cloud-onboarding-remove-alloy-and-finalizer
    app.kubernetes.io/instance: test
    helm.sh/chart: grafana-cloud-onboarding-0.4.0
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
spec:
  ttlSecondsAfterFinished: 300
  backoffLimit: 3
  template:
    metadata:
      name: test-grafana-cloud-onboarding-remove-alloy-and-finalizer
      labels:
        app.kubernetes.io/name: test-grafana-cloud-onboarding
        app.kubernetes.io/instance: test
        linkerd.io/inject: disabled
        sidecar.istio.io/inject: "false"
    spec:
      restartPolicy: Never
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: test-grafana-cloud-onboarding-remove-alloy-and-finalizer
      containers:
        - name: remove-finalizers
          image: "ghcr.io/grafana/helm-chart-toolbox-kubectl:0.1.1"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - -ce
            - |
              echo "Deleting Alloy instance: alloy/test-alloy-daemon..."
              kubectl delete alloy/test-alloy-daemon --ignore-not-found=true --wait
              kubectl wait --for=delete alloy/test-alloy-daemon --timeout=60s || echo "Timed out waiting for deletion of alloy/test-alloy-daemon or it may not exist."

              kubectl patch \
                --namespace=test-namespace \
                --type json \
                --patch='[{"op": "remove", "path": "/metadata/finalizers"}]' \
                deployment/test-alloy-operator
          securityContext:
            runAsNonRoot: true
            runAsUser: 4242
            allowPrivilegeEscalation: false
            capabilities:
              drop: ["ALL"]
            readOnlyRootFilesystem: true
            seccompProfile:
              type: RuntimeDefault
